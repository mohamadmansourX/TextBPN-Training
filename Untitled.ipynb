{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "from util.misc import AverageMeter\n",
    "import torch.utils.data as data\n",
    "from dataset import TotalText, Ctw1500Text, Icdar15Text, Mlt2017Text, TD500Text\n",
    "from network.textnet import TextNet\n",
    "from util.augmentation import BaseTransform,Augmentation\n",
    "from cfglib.config import init_config, update_config, print_config\n",
    "from cfglib.option import BaseOptions\n",
    "from util.visualize import visualize_detection, visualize_gt\n",
    "from util.misc import to_device, mkdirs,rescale_result\n",
    "from util.eval import deal_eval_total_text, deal_eval_ctw1500, deal_eval_icdar15, \\\n",
    "    deal_eval_TD500, data_transfer_ICDAR, data_transfer_TD500, data_transfer_MLT2017\n",
    "import sys\n",
    "from network.loss import  TextLoss\n",
    "\n",
    "sys.argv=['']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(batch_size=12, checkepoch=590, cls_threshold=0.875, cuda=True, dis_threshold=0.3, display_freq=10, exp_name='Totaltext', gamma=0.1, gpu='1', img_root=None, input_size=640, log_dir='./logs/', log_freq=10000, loss='CrossEntropyLoss', lr=0.001, lr_adjust='fix', max_epoch=200, means=(0.485, 0.456, 0.406), mgpu=False, momentum=0.9, net='resnet50', num_workers=8, optim='Adam', pretrain=False, rescale=255.0, resume=None, save_dir='./model/', save_freq=5, start_epoch=0, stds=(0.229, 0.224, 0.225), stepvalues=[], test_size=[640, 1024], val_freq=1000, verbose=True, vis_dir='./vis/', viz=False, viz_freq=50, weight_decay=0.0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "option = BaseOptions()\n",
    "args = option.initialize()\n",
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========Options============\n",
      "adj_num: 4\n",
      "approx_factor: 0.007\n",
      "batch_size: 12\n",
      "checkepoch: 590\n",
      "cls_threshold: 0.875\n",
      "cuda: True\n",
      "dataset: TD500\n",
      "device: cuda\n",
      "dis_threshold: 0.3\n",
      "display_freq: 10\n",
      "epochs: 200\n",
      "exp_name: Totaltext\n",
      "gamma: 0.1\n",
      "global_checkepoch: -1\n",
      "global_dataset: TD500\n",
      "global_epochs: 200\n",
      "global_gpu: [0]\n",
      "global_output_idr: output\n",
      "global_pretrain: False\n",
      "global_pretrain_model: None\n",
      "global_print_freq: 2\n",
      "global_save_freq: 10\n",
      "global_save_path: ./checkpoints/\n",
      "global_use_gpu: False\n",
      "global_val_freq: 1\n",
      "gpu: 1\n",
      "grad_clip: 0\n",
      "img_root: None\n",
      "input_size: 640\n",
      "log_dir: ./logs/\n",
      "log_freq: 10000\n",
      "loss: CrossEntropyLoss\n",
      "lr: 0.001\n",
      "lr_adjust: fix\n",
      "max_annotation: 64\n",
      "max_epoch: 200\n",
      "max_points: 20\n",
      "means: [0.485, 0.456, 0.406]\n",
      "mgpu: False\n",
      "momentum: 0.9\n",
      "net: resnet50\n",
      "num_points: 20\n",
      "num_workers: 8\n",
      "optim: Adam\n",
      "output_dir: output\n",
      "output_idr: output\n",
      "pretrain: False\n",
      "pretrain_model: None\n",
      "print_freq: 2\n",
      "rescale: 255.0\n",
      "resume: None\n",
      "save_dir: ./model/\n",
      "save_freq: 5\n",
      "save_path: ./checkpoints/\n",
      "scale: 1\n",
      "start_epoch: 0\n",
      "stds: [0.229, 0.224, 0.225]\n",
      "stepvalues: []\n",
      "test_size: [640, 1024]\n",
      "train_adj_num: 4\n",
      "train_approx_factor: 0.007\n",
      "train_batch_size: 12\n",
      "train_cls_threshold: 0.8\n",
      "train_data_root: ./TD500\n",
      "train_dis_threshold: 0.3\n",
      "train_gamma: 0.1\n",
      "train_grad_clip: 0\n",
      "train_input_size: 512\n",
      "train_lr: 0.0001\n",
      "train_max_annotation: 64\n",
      "train_max_points: 20\n",
      "train_num_points: 20\n",
      "train_num_workers: 0\n",
      "train_rgb_mean: [0.485, 0.456, 0.406]\n",
      "train_rgb_std: [0.229, 0.224, 0.225]\n",
      "train_scale: 1\n",
      "train_shuffle: True\n",
      "train_step_size: 20\n",
      "train_use_hard: True\n",
      "use_gpu: False\n",
      "use_hard: True\n",
      "val_batch_size: 1\n",
      "val_data_root: ./TD500\n",
      "val_freq: 1000\n",
      "val_input_size: 512\n",
      "val_num_workers: 1\n",
      "val_rgb_mean: [0.485, 0.456, 0.406]\n",
      "val_rgb_std: [0.229, 0.224, 0.225]\n",
      "val_shuffle: False\n",
      "verbose: True\n",
      "vis_dir: ./vis/\n",
      "viz: False\n",
      "viz_freq: 50\n",
      "wandb_flag: False\n",
      "wandb_name: ./TD500\n",
      "wandb_project: TextBPN\n",
      "weight_decay: 0.0\n",
      "=============End=============\n",
      "load the resnet50 weight from ./cache\n",
      "torch.Size([1, 400, 2])\n"
     ]
    }
   ],
   "source": [
    "cfg = init_config()\n",
    "option = BaseOptions()\n",
    "args = option.initialize()\n",
    "update_config(cfg, args)\n",
    "print_config(cfg)\n",
    "\n",
    "# Create checkpoint directory\n",
    "if not os.path.exists(cfg.save_path):\n",
    "    mkdirs(cfg.save_path)\n",
    "\n",
    "# Create the model\n",
    "model = TextNet(is_training=True, backbone=cfg.net,)\n",
    "model.train()\n",
    "# Initialize wandb\n",
    "if cfg.wandb_flag:\n",
    "    global wandb\n",
    "    import wandb\n",
    "    wandb.init(project=cfg.wandb_project, name=cfg.wandb_name, config=cfg)\n",
    "    wandb.watch(model)\n",
    "\n",
    "# Load the dataset\n",
    "if cfg.dataset == 'TotalText':\n",
    "    train_dataset = TotalText(cfg.train_data_root, cfg.train_data_list,\n",
    "                            transform=Augmentation(cfg.input_size, cfg.rgb_mean, cfg.rgb_std, cfg.inter_type))\n",
    "    val_dataset = TotalText(cfg.val_data_root, cfg.val_data_list,\n",
    "                            transform=BaseTransform(cfg.input_size, cfg.rgb_mean, cfg.rgb_std, cfg.inter_type))\n",
    "elif cfg.dataset == 'CTW1500':\n",
    "    train_dataset = Ctw1500Text(cfg.train_data_root, cfg.train_data_list,\n",
    "                                transform=Augmentation(cfg.input_size, cfg.rgb_mean, cfg.rgb_std, cfg.inter_type))\n",
    "    val_dataset = Ctw1500Text(cfg.val_data_root, cfg.val_data_list,\n",
    "                            transform=BaseTransform(cfg.input_size, cfg.rgb_mean, cfg.rgb_std, cfg.inter_type))\n",
    "elif cfg.dataset == 'ICDAR15':\n",
    "    train_dataset = Icdar15Text(cfg.train_data_root, cfg.train_data_list,\n",
    "                                transform=Augmentation(cfg.input_size, cfg.rgb_mean, cfg.rgb_std, cfg.inter_type))\n",
    "    val_dataset = Icdar15Text(cfg.val_data_root, cfg.val_data_list,\n",
    "                            transform=BaseTransform(cfg.input_size, cfg.rgb_mean, cfg.rgb_std, cfg.inter_type))\n",
    "elif cfg.dataset == 'MLT2017':\n",
    "    train_dataset = Mlt2017Text(cfg.train_data_root, cfg.train_data_list,\n",
    "                                transform=Augmentation(cfg.input_size, cfg.rgb_mean, cfg.rgb_std, cfg.inter_type))\n",
    "    val_dataset = Mlt2017Text(cfg.val_data_root, cfg.val_data_list,\n",
    "                            transform=BaseTransform(cfg.input_size, cfg.rgb_mean, cfg.rgb_std, cfg.inter_type))\n",
    "elif cfg.dataset == 'TD500':\n",
    "    train_dataset = TD500Text(cfg.train_data_root, is_training=True,\n",
    "                            transform=Augmentation(cfg.train_input_size, cfg.train_rgb_mean, cfg.train_rgb_std))\n",
    "    val_dataset = TD500Text(cfg.val_data_root, is_training=False,\n",
    "                            transform=BaseTransform(cfg.val_input_size, cfg.val_rgb_mean, cfg.val_rgb_std))\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "\n",
    "# Create the dataloader\n",
    "train_loader = data.DataLoader(train_dataset, batch_size=cfg.train_batch_size,\n",
    "                                shuffle=cfg.train_shuffle, num_workers=cfg.train_num_workers) #, pin_memory=True)\n",
    "val_loader = data.DataLoader(val_dataset, batch_size=cfg.val_batch_size,\n",
    "                                shuffle=cfg.val_shuffle, num_workers=cfg.val_num_workers) #, pin_memory=True)\n",
    "\n",
    "# Create the optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=cfg.train_lr)\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=cfg.lr, momentum=cfg.momentum,\n",
    "#                            weight_decay=cfg.weight_decay, nesterov=cfg.nesterov)\n",
    "\n",
    "# Create the learning rate scheduler\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=cfg.train_step_size, gamma=cfg.train_gamma)\n",
    "\n",
    "# Create the loss criterion\n",
    "criterion = TextLoss()\n",
    "\n",
    "# Load the pretrained model\n",
    "if cfg.pretrain:\n",
    "    print('Loading pretrained model from {}'.format(cfg.pretrain_model))\n",
    "    model.load_state_dict(torch.load(cfg.pretrain_model))\n",
    "\n",
    "# Move the model to GPU\n",
    "if cfg.use_gpu:\n",
    "    model = model.cuda()\n",
    "    criterion = criterion.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.val_input_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = TD500Text(\n",
    "    cfg.val_data_root, \n",
    "    is_training=False,\n",
    "    transform=BaseTransform(cfg.test_size, cfg.val_rgb_mean, cfg.val_rgb_std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = data.DataLoader(val_dataset, batch_size=1, shuffle=False, num_workers=cfg.num_workers)\n",
    "for i, (image, meta) in enumerate(test_loader):\n",
    "    if 'ignore_tags' in meta:\n",
    "        print(\"JDHCAGDYG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = data.DataLoader(testset, batch_size=1, shuffle=False, num_workers=cfg.num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (image, meta) in enumerate(test_loader):\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
